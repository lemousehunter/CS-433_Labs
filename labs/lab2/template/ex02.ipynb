{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    loss = 1 / (2 * N) * e.T.dot(e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694.4833658870834"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(y, tx, np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "\n",
    "    for idx_w0 in range(len(grid_w0)):\n",
    "        for idx_w1 in range(len(grid_w1)):\n",
    "            losses[idx_w0][idx_w1] = compute_loss(y, tx, np.array([grid_w0[idx_w0], grid_w1[idx_w1]]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.497 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot_10\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    #print(\"tx.shape:\", tx.shape)\n",
    "    #print(\"w.shape:\", w.shape)\n",
    "    e = y - tx.dot(w)\n",
    "    #print(\"e.shape:\", e.shape)\n",
    "    \n",
    "    gradient = (-1 / N) * tx.T.dot(e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient for wT = (100,20): [26.706078    6.52028757], norm: 27.490521129292496\n",
      "gradient for wT = (50,10): [-23.293922    -3.47971243], norm: 23.552392678247728\n"
     ]
    }
   ],
   "source": [
    "m100_20 = compute_gradient(y, tx, np.array([100,20]))\n",
    "m50_10 = compute_gradient(y, tx, np.array([50,10]))\n",
    "norm100_20 = np.linalg.norm(m100_20)\n",
    "norm50_10 = np.linalg.norm(m50_10)\n",
    "print(f\"gradient for wT = (100,20): {m100_20}, norm: {norm100_20}\")\n",
    "print(f\"gradient for wT = (50,10): {m50_10}, norm: {norm50_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradients = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w by gradient    \n",
    "        w = w - gamma * gradients\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759168, w0=7.32939220021052, w1=1.3479712434988977\n",
      "GD iter. 1/49: loss=2264.635056030003, w0=13.925845180399987, w1=2.5611453626479044\n",
      "GD iter. 2/49: loss=1837.27771407938, w0=19.862652862570506, w1=3.653002069882012\n",
      "GD iter. 3/49: loss=1491.1182670993758, w0=25.205779776523975, w1=4.63567310639271\n",
      "GD iter. 4/49: loss=1210.729115045572, w0=30.014593999082095, w1=5.520077039252339\n",
      "GD iter. 5/49: loss=983.613901881991, w0=34.3425267993844, w1=6.316040578826005\n",
      "GD iter. 6/49: loss=799.6505792194903, w0=38.237666319656476, w1=7.032407764442305\n",
      "GD iter. 7/49: loss=650.640287862865, w0=41.74329188790135, w1=7.677138231496976\n",
      "GD iter. 8/49: loss=529.9419518639982, w0=44.89835489932173, w1=8.25739565184618\n",
      "GD iter. 9/49: loss=432.1762997049161, w0=47.737911609600076, w1=8.779627330160462\n",
      "GD iter. 10/49: loss=352.9861214560597, w0=50.29351264885059, w1=9.249635840643318\n",
      "GD iter. 11/49: loss=288.842077074486, w0=52.59355358417605, w1=9.672643500077887\n",
      "GD iter. 12/49: loss=236.8854011254112, w0=54.66359042596896, w1=10.053350393569001\n",
      "GD iter. 13/49: loss=194.8004936066607, w0=56.526623583582584, w1=10.395986597711005\n",
      "GD iter. 14/49: loss=160.71171851647279, w0=58.20335342543484, w1=10.704359181438807\n",
      "GD iter. 15/49: loss=133.09981069342058, w0=59.712410283101875, w1=10.981894506793829\n",
      "GD iter. 16/49: loss=110.73416535674828, w0=61.070561455002206, w1=11.23167629961335\n",
      "GD iter. 17/49: loss=92.6179926340437, w0=62.2928975097125, w1=11.456479913150918\n",
      "GD iter. 18/49: loss=77.94389272865301, w0=63.39299995895177, w1=11.65880316533473\n",
      "GD iter. 19/49: loss=66.05787180528655, w0=64.38309216326711, w1=11.84089409230016\n",
      "GD iter. 20/49: loss=56.430194857359716, w0=65.27417514715091, w1=12.00477592656905\n",
      "GD iter. 21/49: loss=48.63177652953898, w0=66.07614983264634, w1=12.15226957741105\n",
      "GD iter. 22/49: loss=42.31505768400418, w0=66.79792704959222, w1=12.285013863168848\n",
      "GD iter. 23/49: loss=37.19851541912101, w0=67.44752654484351, w1=12.404483720350868\n",
      "GD iter. 24/49: loss=33.054116184565615, w0=68.03216609056967, w1=12.512006591814686\n",
      "GD iter. 25/49: loss=29.69715280457577, w0=68.55834168172322, w1=12.608777176132122\n",
      "GD iter. 26/49: loss=26.978012466783987, w0=69.03189971376142, w1=12.695870702017814\n",
      "GD iter. 27/49: loss=24.77550879317261, w0=69.45810194259579, w1=12.774254875314936\n",
      "GD iter. 28/49: loss=22.991480817547423, w0=69.84168394854673, w1=12.844800631282347\n",
      "GD iter. 29/49: loss=21.546418157290997, w0=70.18690775390257, w1=12.908291811653017\n",
      "GD iter. 30/49: loss=20.375917402483296, w0=70.49760917872284, w1=12.965433873986619\n",
      "GD iter. 31/49: loss=19.42781179108905, w0=70.77724046106107, w1=13.016861730086863\n",
      "GD iter. 32/49: loss=18.65984624585973, w0=71.02890861516548, w1=13.063146800577082\n",
      "GD iter. 33/49: loss=18.037794154223974, w0=71.25540995385944, w1=13.104803364018277\n",
      "GD iter. 34/49: loss=17.53393195999902, w0=71.45926115868401, w1=13.142294271115354\n",
      "GD iter. 35/49: loss=17.1258035826768, w0=71.64272724302613, w1=13.176036087502723\n",
      "GD iter. 36/49: loss=16.795219597045797, w0=71.80784671893403, w1=13.206403722251356\n",
      "GD iter. 37/49: loss=16.527446568684688, w0=71.95645424725114, w1=13.233734593525124\n",
      "GD iter. 38/49: loss=16.31055041571219, w0=72.09020102273655, w1=13.258332377671517\n",
      "GD iter. 39/49: loss=16.134864531804453, w0=72.21057312067342, w1=13.28047038340327\n",
      "GD iter. 40/49: loss=15.99255896583919, w0=72.31890800881659, w1=13.300394588561847\n",
      "GD iter. 41/49: loss=15.877291457407335, w0=72.41640940814546, w1=13.318326373204567\n",
      "GD iter. 42/49: loss=15.783924775577523, w0=72.50416066754143, w1=13.334464979383014\n",
      "GD iter. 43/49: loss=15.708297763295382, w0=72.5831368009978, w1=13.348989724943618\n",
      "GD iter. 44/49: loss=15.647039883346839, w0=72.65421532110854, w1=13.36206199594816\n",
      "GD iter. 45/49: loss=15.597421000588534, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=15.557229705554297, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=15.524674756576564, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=15.498305247904606, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=15.476945945880313, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.493 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6284d754a64982b4262838c55f0275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    B = y.shape[0]\n",
    "    gradients = np.zeros((B, 2))\n",
    "    for idx in range(B):\n",
    "        gradients[idx] = compute_gradient(np.array([y[idx]]), np.array([tx[idx, :]]), w)\n",
    "    return np.average(gradients, axis=0)\n",
    "\n",
    "\n",
    "def compute_stoch_gradient_eff(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    B = y.shape[0]\n",
    "    gradients = compute_gradient(y, tx, w)\n",
    "    return gradients\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    # Get batched data\n",
    "    iter = 0\n",
    "    for batched_y, batched_tx in batch_iter(y, tx, batch_size, max_iters):\n",
    "        # Get gradient\n",
    "        gradient = compute_stoch_gradient(batched_y, batched_tx, w)\n",
    "        \n",
    "        # Stochastic Gradient Descent\n",
    "        w = w - gamma * gradient\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # Track losses and weights histories\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )        \n",
    "        )\n",
    "        iter += 1\n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent_eff(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    # Get batched data\n",
    "    iter = 0\n",
    "    for batched_y, batched_tx in batch_iter(y, tx, batch_size, max_iters):\n",
    "        # Get gradient\n",
    "        gradient = compute_stoch_gradient_eff(batched_y, batched_tx, w)\n",
    "        \n",
    "        # Stochastic Gradient Descent\n",
    "        w = w - gamma * gradient\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # Track losses and weights histories\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )        \n",
    "        )\n",
    "        iter += 1\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-72.293922  , -11.47971243])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stoch_gradient(y, tx, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-72.293922  , -11.47971243])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stoch_gradient_eff(y, tx, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2602.320802834684, w0=4.979855251077027, w1=-9.038238490167988\n",
      "SGD iter. 1/49: loss=2518.5663458642716, w0=7.924011480285464, w1=-13.596766552504338\n",
      "SGD iter. 2/49: loss=2235.5807685783916, w0=13.432707582222875, w1=-15.795272955203744\n",
      "SGD iter. 3/49: loss=1423.7309527043749, w0=23.095335022584415, w1=-3.7479396084723593\n",
      "SGD iter. 4/49: loss=1350.232695135909, w0=26.071888544593776, w1=-7.491056999436422\n",
      "SGD iter. 5/49: loss=1123.0274699660588, w0=31.050421671538956, w1=-7.275283204998724\n",
      "SGD iter. 6/49: loss=899.9101714970552, w0=36.26026034039439, w1=-6.4591057642519765\n",
      "SGD iter. 7/49: loss=764.5523357764288, w0=40.22307897983592, w1=-6.6362572809263005\n",
      "SGD iter. 8/49: loss=401.71879631283997, w0=46.86527531484599, w1=4.866208434406887\n",
      "SGD iter. 9/49: loss=355.28954977512535, w0=48.79964998524861, w1=4.544503306108578\n",
      "SGD iter. 10/49: loss=255.47524320194816, w0=51.99098950481208, w1=8.34514549297624\n",
      "SGD iter. 11/49: loss=187.30163006954137, w0=54.77646421592605, w1=12.512633566179762\n",
      "SGD iter. 12/49: loss=155.35180080030446, w0=56.66998871693196, w1=15.370920512663131\n",
      "SGD iter. 13/49: loss=135.8578153354736, w0=58.397403207353804, w1=17.842920283683277\n",
      "SGD iter. 14/49: loss=100.73112762635887, w0=60.31969970123682, w1=15.01595304202585\n",
      "SGD iter. 15/49: loss=82.79906257407093, w0=62.16562060840406, w1=16.79441566475676\n",
      "SGD iter. 16/49: loss=69.28630332684968, w0=63.43849748419458, w1=16.746429209949323\n",
      "SGD iter. 17/49: loss=71.86320510306179, w0=62.928529318167236, w1=15.827747565950734\n",
      "SGD iter. 18/49: loss=66.47595957488106, w0=63.40918798605359, w1=15.594464629459047\n",
      "SGD iter. 19/49: loss=60.91668228032282, w0=64.35792031087973, w1=16.827765988533642\n",
      "SGD iter. 20/49: loss=51.88180428245841, w0=65.5756127312243, w1=17.142980760718412\n",
      "SGD iter. 21/49: loss=28.005316116356084, w0=68.27146534190008, w1=13.36230037677214\n",
      "SGD iter. 22/49: loss=27.768835073850187, w0=68.31766551716827, w1=13.427121502645369\n",
      "SGD iter. 23/49: loss=27.530429589919315, w0=68.36553481391431, w1=13.470592925363087\n",
      "SGD iter. 24/49: loss=22.221375180026048, w0=69.6717230160672, w1=14.221769796611914\n",
      "SGD iter. 25/49: loss=21.414622552717507, w0=69.88285221840738, w1=14.129383350545522\n",
      "SGD iter. 26/49: loss=19.762378470878573, w0=70.61927114592278, w1=14.744316721562933\n",
      "SGD iter. 27/49: loss=18.750948989911855, w0=71.1031322676959, w1=14.869159296791171\n",
      "SGD iter. 28/49: loss=19.52166260048902, w0=70.67158390635345, w1=14.660768053869191\n",
      "SGD iter. 29/49: loss=19.772652344021772, w0=70.58136053811887, w1=14.669476804299686\n",
      "SGD iter. 30/49: loss=18.524952702217295, w0=71.50032717023778, w1=15.229325891119949\n",
      "SGD iter. 31/49: loss=18.607225342164433, w0=71.14229085969666, w1=14.826248128421577\n",
      "SGD iter. 32/49: loss=18.267761467430983, w0=71.61828151339672, w1=15.199007682991558\n",
      "SGD iter. 33/49: loss=15.830915801807636, w0=72.60241883957018, w1=12.837934247325625\n",
      "SGD iter. 34/49: loss=16.678292925269204, w0=71.69625280705363, w1=13.3000926808892\n",
      "SGD iter. 35/49: loss=18.50307395666698, w0=70.83450444043957, w1=13.910569062007731\n",
      "SGD iter. 36/49: loss=19.365732359645172, w0=70.5796355746484, w1=14.249347524726666\n",
      "SGD iter. 37/49: loss=16.88137707003705, w0=71.66221237173507, w1=12.906561421230286\n",
      "SGD iter. 38/49: loss=15.855985117388162, w0=72.36699048927036, w1=13.19512067890455\n",
      "SGD iter. 39/49: loss=16.09837764515631, w0=73.14340964584164, w1=12.295514147848833\n",
      "SGD iter. 40/49: loss=17.207742070812376, w0=72.43287937639487, w1=11.77609451988137\n",
      "SGD iter. 41/49: loss=18.08558273325213, w0=71.77393032578757, w1=11.722153038333952\n",
      "SGD iter. 42/49: loss=17.834103522663423, w0=71.90047030607612, w1=11.76078146531582\n",
      "SGD iter. 43/49: loss=17.869866213045817, w0=71.7026222413181, w1=11.919032531949423\n",
      "SGD iter. 44/49: loss=17.88997206963339, w0=71.74473322747116, w1=11.86472557988592\n",
      "SGD iter. 45/49: loss=17.841834833815255, w0=72.0299132160953, w1=11.659224665499368\n",
      "SGD iter. 46/49: loss=17.98546333357265, w0=71.6014968485318, w1=11.951691483222163\n",
      "SGD iter. 47/49: loss=18.93575970779416, w0=71.07261751025837, w1=12.008131641274119\n",
      "SGD iter. 48/49: loss=16.955139036133815, w0=71.6812194102441, w1=12.746437121881243\n",
      "SGD iter. 49/49: loss=15.776804010974011, w0=72.42778766078304, w1=13.301825990672263\n",
      "SGD: execution time=0.455 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2168.348012570423, w0=8.250307578378374, w1=4.80489416019242\n",
      "SGD iter. 1/49: loss=1670.5135964042627, w0=15.805632272743377, w1=15.793143018304583\n",
      "SGD iter. 2/49: loss=1389.1498068108415, w0=21.102314771209105, w1=18.333986071682663\n",
      "SGD iter. 3/49: loss=1142.5262383129543, w0=26.53788388830033, w1=21.735231848610226\n",
      "SGD iter. 4/49: loss=903.062115459786, w0=31.886348745806064, w1=21.2749245684806\n",
      "SGD iter. 5/49: loss=771.848883499654, w0=35.593271009303585, w1=23.049817240089954\n",
      "SGD iter. 6/49: loss=760.6860464128762, w0=36.770191496776405, w1=25.994400887451935\n",
      "SGD iter. 7/49: loss=521.1742711842046, w0=41.85959140665464, w1=18.323226308890025\n",
      "SGD iter. 8/49: loss=458.7369888456416, w0=44.302401030826346, w1=20.276322893557566\n",
      "SGD iter. 9/49: loss=417.3585424873315, w0=46.67255887923918, w1=23.23923776758148\n",
      "SGD iter. 10/49: loss=323.86821718598765, w0=49.680653623274864, w1=21.185438559349286\n",
      "SGD iter. 11/49: loss=271.09663772605927, w0=52.29520396449535, w1=21.87467166606447\n",
      "SGD iter. 12/49: loss=154.1799964277846, w0=56.81322765422227, w1=11.03534525112113\n",
      "SGD iter. 13/49: loss=113.84690476453146, w0=59.37713408008548, w1=11.678310834864686\n",
      "SGD iter. 14/49: loss=96.63586952885873, w0=61.50577034325995, w1=8.627965967156776\n",
      "SGD iter. 15/49: loss=80.01008919172625, w0=63.12247633231349, w1=8.401317449142686\n",
      "SGD iter. 16/49: loss=78.97061772452362, w0=63.209683538085145, w1=8.432178947020182\n",
      "SGD iter. 17/49: loss=78.53413465295223, w0=63.380464034531094, w1=8.186334997720149\n",
      "SGD iter. 18/49: loss=78.83801600317253, w0=63.08917044873606, w1=8.708203029916401\n",
      "SGD iter. 19/49: loss=56.70985163941598, w0=64.89768692141439, w1=9.993860457215117\n",
      "SGD iter. 20/49: loss=57.94582720371839, w0=64.73782337262931, w1=10.028183076507657\n",
      "SGD iter. 21/49: loss=55.65319976378122, w0=65.14655037907961, w1=9.717404708842595\n",
      "SGD iter. 22/49: loss=48.873455509378005, w0=66.05313310043961, w1=9.665775969522715\n",
      "SGD iter. 23/49: loss=47.06099407951864, w0=66.46638217878198, w1=9.388879710186641\n",
      "SGD iter. 24/49: loss=47.25817804853161, w0=66.55910580224392, w1=9.1917255980817\n",
      "SGD iter. 25/49: loss=36.57482701572695, w0=67.54303262610006, w1=10.429278092350865\n",
      "SGD iter. 26/49: loss=30.477143686206883, w0=68.4901163826412, w1=10.814011175440028\n",
      "SGD iter. 27/49: loss=30.40713567988119, w0=68.51228552909258, w1=10.800449800968423\n",
      "SGD iter. 28/49: loss=24.883406193210938, w0=70.10045761539561, w1=10.513768763983022\n",
      "SGD iter. 29/49: loss=25.347291249789073, w0=69.96880863038166, w1=10.50205768215004\n",
      "SGD iter. 30/49: loss=20.75103665509208, w0=70.87739905989004, w1=11.268216461157488\n",
      "SGD iter. 31/49: loss=20.352253625953136, w0=70.51649923450395, w1=11.990197653505575\n",
      "SGD iter. 32/49: loss=21.874931795922638, w0=70.17256375665555, w1=11.681043327634939\n",
      "SGD iter. 33/49: loss=17.931465338252064, w0=71.15755393850797, w1=12.753705247912219\n",
      "SGD iter. 34/49: loss=17.19311005117333, w0=71.40480530034, w1=13.265977899174922\n",
      "SGD iter. 35/49: loss=16.76702876910907, w0=71.63295853662372, w1=13.538722323102197\n",
      "SGD iter. 36/49: loss=15.459650812298555, w0=72.95157622658445, w1=13.653853920343378\n",
      "SGD iter. 37/49: loss=16.057133741111663, w0=72.19582767829374, w1=13.11000903957916\n",
      "SGD iter. 38/49: loss=16.439508716598013, w0=71.95150088134483, w1=12.92731110436635\n",
      "SGD iter. 39/49: loss=16.030141065371755, w0=72.23266400561879, w1=13.076924825971119\n",
      "SGD iter. 40/49: loss=15.580829089113093, w0=72.69609226117416, w1=13.299484686221794\n",
      "SGD iter. 41/49: loss=15.841669243586402, w0=73.16616020090554, w1=12.533541376634258\n",
      "SGD iter. 42/49: loss=15.967783609124897, w0=73.04215818036852, w1=12.430709833154555\n",
      "SGD iter. 43/49: loss=15.528090712678162, w0=73.73596290163404, w1=13.778050382359568\n",
      "SGD iter. 44/49: loss=15.610435443789886, w0=73.95361253368075, w1=13.597625761082929\n",
      "SGD iter. 45/49: loss=15.533208492322721, w0=73.73732985076296, w1=13.16661384550752\n",
      "SGD iter. 46/49: loss=19.25983393629134, w0=75.09274522860603, w1=11.35553562155638\n",
      "SGD iter. 47/49: loss=18.035487831815647, w0=74.09017882687533, w1=11.3198108180214\n",
      "SGD iter. 48/49: loss=17.74525726948711, w0=74.22068004418762, w1=11.515060218246617\n",
      "SGD iter. 49/49: loss=17.750999411404237, w0=74.22853456554981, w1=11.515858458390698\n",
      "SGD: execution time=0.169 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent_eff(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828eb9bcacbf4693a84e9ce4cb255d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sgd_ws))\n",
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.835114535854, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.28212470159497, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889339, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339528, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260396, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260339, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.007 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0a4208f76b424080dff91f94e84236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    differentials = np.zeros(e.shape)\n",
    "        \n",
    "    for i in range(len(e)):\n",
    "        if e[i] < 0:\n",
    "            differentials[i] = -1.0\n",
    "        elif e[i] == 0:\n",
    "            differentials[i] = np.random.uniform(-1.0, 1.0)  # getting any subgradient for when e == 0, since all lines for y = c, with c being any value between -1.0 and 1.0 is considered a subgradient of the absolute function\n",
    "        else:  # e[i] > 0\n",
    "            differentials[i] = 1.0\n",
    "    #print(\"subgradient shape:\", differentials.shape)\n",
    "\n",
    "    return -(1/N)*differentials.dot(tx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subgradient: [-1.00000000e+00 -1.25092456e-15]\n"
     ]
    }
   ],
   "source": [
    "s = compute_subgradient_mae(y, tx, np.array([1,2]))\n",
    "print(\"subgradient:\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    return np.absolute((1 / N) * np.sum(e))\n",
    "\n",
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute subgradient\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_mae_loss(y, tx, w)\n",
    "        \n",
    "        # Update w with subgradient\n",
    "        w = w - gamma * subgradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.96780585492635, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.16780585492634, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.47473654799564, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.788597934134252, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.102459320272864, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.4371127856194, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.778696944035243, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.120281102451084, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.475726647005537, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=21.845033577698608, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.221271201460983, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.61137021136198, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.00839991433228, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.41236031037188, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=18.82325139948079, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.24800387472831, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=17.700479122253064, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.16681575591643, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=16.640083082649102, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=16.134142488589703, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=15.635132587599605, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=15.156914765817426, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=14.68562763710455, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=14.228201894530294, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=13.80542961730257, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=13.389588033144156, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=12.980677142055047, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=12.57176625096594, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=12.176716746015448, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=11.788597934134264, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=11.414340508391684, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=11.047013775718414, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=10.686617736114451, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=10.326221696510489, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=9.979687043045141, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=9.640083082649106, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=9.300479122253074, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=8.96087516185704, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=8.621271201461006, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=8.302459320272881, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=7.9975088252233695, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=7.692558330173859, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=7.401469221262963, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=7.110380112352067, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=6.833152389579785, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=6.555924666807503, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=6.278696944035223, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=6.0014692212629415, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=5.738102884629273, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=5.474736547995608, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=5.211370211361941, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=4.954934567797582, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=4.712360310371837, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=4.483647439084708, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=4.261865260866886, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=4.040083082649064, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=3.811370211361933, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=3.5895880331441115, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=3.3747365479955973, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=3.1737464489856966, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=2.979687043045104, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=2.792558330173818, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=2.6054296173025335, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=2.446023676708477, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=2.314340508391651, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=2.1965187262134402, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=2.113350409381751, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=2.0301820925500627, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=1.9608751618569886, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=1.8915682311639146, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=1.8222613004708408, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=1.7668157559163817, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=1.7183009044312296, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=1.6697860529460775, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=1.635132587599541, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=1.6074098153223113, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=1.5796870430450816, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=1.5658256569064666, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=1.5450335776985447, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=1.5173108054213154, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=1.5034494192827002, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=1.4826573400747785, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=1.4618652608668559, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=1.4410731816589342, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=1.4272117955203194, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=1.4064197163123968, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=1.39948902324309, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=1.3925583301737825, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=1.3856276371044753, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=1.385627637104474, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=1.3856276371044747, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=1.3925583301737823, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=1.3925583301737823, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=1.3925583301737816, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=1.3994890232430894, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=1.3994890232430897, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=1.3994890232430888, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=1.3994890232430897, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=1.4064197163123962, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=1.4064197163123973, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=1.4064197163123968, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=1.4133504093817044, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=1.4133504093817038, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=1.4133504093817044, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=1.4133504093817042, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=1.4202811024510116, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=1.420281102451012, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=1.4202811024510118, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=1.427211795520319, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=1.4272117955203192, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=1.4272117955203185, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=1.4272117955203194, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=1.434142488589626, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=1.4341424885896266, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=1.434142488589627, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=1.4410731816589344, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=1.4410731816589344, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=1.4341424885896275, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=1.4410731816589344, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.079 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5514890670c446d9eff7eae90db74bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_subgradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    return compute_subgradient_mae(y, tx, w)\n",
    "\n",
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    n_iter = 0\n",
    "\n",
    "    for batched_y, batched_x in batch_iter(y, tx, batch_size, max_iters):\n",
    "        \n",
    "        # Compute subgradient and loss\n",
    "        subgradient = compute_stoch_subgradient(y, tx, w)\n",
    "        loss = compute_mae_loss(y, tx, w)\n",
    "        \n",
    "        \n",
    "        # Update weights with computed subgradients\n",
    "        w = w - gamma * subgradient\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Update weights and loss history\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        n_iter += 1\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubSGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubSGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubSGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=5.253883137127127e-15\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubSGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubSGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubSGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubSGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubSGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubSGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubSGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubSGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubSGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubSGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubSGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubSGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubSGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubSGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubSGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubSGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubSGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubSGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubSGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubSGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubSGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubSGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubSGD iter. 63/499: loss=29.96780585492635, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubSGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubSGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubSGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubSGD iter. 67/499: loss=27.16780585492634, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubSGD iter. 68/499: loss=26.47473654799564, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubSGD iter. 69/499: loss=25.788597934134252, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubSGD iter. 70/499: loss=25.102459320272864, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubSGD iter. 71/499: loss=24.4371127856194, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubSGD iter. 72/499: loss=23.778696944035243, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubSGD iter. 73/499: loss=23.120281102451084, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubSGD iter. 74/499: loss=22.475726647005537, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubSGD iter. 75/499: loss=21.845033577698608, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubSGD iter. 76/499: loss=21.221271201460983, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubSGD iter. 77/499: loss=20.61137021136198, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubSGD iter. 78/499: loss=20.00839991433228, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubSGD iter. 79/499: loss=19.41236031037188, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubSGD iter. 80/499: loss=18.82325139948079, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubSGD iter. 81/499: loss=18.24800387472831, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubSGD iter. 82/499: loss=17.700479122253064, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubSGD iter. 83/499: loss=17.16681575591643, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubSGD iter. 84/499: loss=16.640083082649102, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubSGD iter. 85/499: loss=16.134142488589703, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubSGD iter. 86/499: loss=15.635132587599605, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubSGD iter. 87/499: loss=15.156914765817426, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubSGD iter. 88/499: loss=14.68562763710455, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubSGD iter. 89/499: loss=14.228201894530294, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubSGD iter. 90/499: loss=13.80542961730257, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubSGD iter. 91/499: loss=13.389588033144156, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubSGD iter. 92/499: loss=12.980677142055047, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubSGD iter. 93/499: loss=12.57176625096594, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubSGD iter. 94/499: loss=12.176716746015448, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubSGD iter. 95/499: loss=11.788597934134264, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubSGD iter. 96/499: loss=11.414340508391684, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubSGD iter. 97/499: loss=11.047013775718414, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubSGD iter. 98/499: loss=10.686617736114451, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubSGD iter. 99/499: loss=10.326221696510489, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubSGD iter. 100/499: loss=9.979687043045141, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubSGD iter. 101/499: loss=9.640083082649106, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubSGD iter. 102/499: loss=9.300479122253074, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubSGD iter. 103/499: loss=8.96087516185704, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubSGD iter. 104/499: loss=8.621271201461006, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubSGD iter. 105/499: loss=8.302459320272881, w0=66.070297029703, w1=8.073669686866932\n",
      "SubSGD iter. 106/499: loss=7.9975088252233695, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubSGD iter. 107/499: loss=7.692558330173859, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubSGD iter. 108/499: loss=7.401469221262963, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubSGD iter. 109/499: loss=7.110380112352067, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubSGD iter. 110/499: loss=6.833152389579785, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubSGD iter. 111/499: loss=6.555924666807503, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubSGD iter. 112/499: loss=6.278696944035223, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubSGD iter. 113/499: loss=6.0014692212629415, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubSGD iter. 114/499: loss=5.738102884629273, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubSGD iter. 115/499: loss=5.474736547995608, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubSGD iter. 116/499: loss=5.211370211361941, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubSGD iter. 117/499: loss=4.954934567797582, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubSGD iter. 118/499: loss=4.712360310371837, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubSGD iter. 119/499: loss=4.483647439084708, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubSGD iter. 120/499: loss=4.261865260866886, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubSGD iter. 121/499: loss=4.040083082649064, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubSGD iter. 122/499: loss=3.811370211361933, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubSGD iter. 123/499: loss=3.5895880331441115, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubSGD iter. 124/499: loss=3.3747365479955973, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubSGD iter. 125/499: loss=3.1737464489856966, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubSGD iter. 126/499: loss=2.979687043045104, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubSGD iter. 127/499: loss=2.792558330173818, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubSGD iter. 128/499: loss=2.6054296173025335, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubSGD iter. 129/499: loss=2.446023676708477, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubSGD iter. 130/499: loss=2.314340508391651, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubSGD iter. 131/499: loss=2.1965187262134402, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubSGD iter. 132/499: loss=2.113350409381751, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubSGD iter. 133/499: loss=2.0301820925500627, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubSGD iter. 134/499: loss=1.9608751618569886, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubSGD iter. 135/499: loss=1.8915682311639146, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubSGD iter. 136/499: loss=1.8222613004708408, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubSGD iter. 137/499: loss=1.7668157559163817, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubSGD iter. 138/499: loss=1.7183009044312296, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubSGD iter. 139/499: loss=1.6697860529460775, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubSGD iter. 140/499: loss=1.635132587599541, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubSGD iter. 141/499: loss=1.6074098153223113, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubSGD iter. 142/499: loss=1.5796870430450816, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubSGD iter. 143/499: loss=1.5658256569064666, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubSGD iter. 144/499: loss=1.5450335776985447, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubSGD iter. 145/499: loss=1.5173108054213154, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubSGD iter. 146/499: loss=1.5034494192827002, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubSGD iter. 147/499: loss=1.4826573400747785, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubSGD iter. 148/499: loss=1.4618652608668559, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubSGD iter. 149/499: loss=1.4410731816589342, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubSGD iter. 150/499: loss=1.4272117955203194, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubSGD iter. 151/499: loss=1.4064197163123968, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubSGD iter. 152/499: loss=1.39948902324309, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubSGD iter. 153/499: loss=1.3925583301737825, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubSGD iter. 154/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubSGD iter. 155/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubSGD iter. 156/499: loss=1.3856276371044753, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubSGD iter. 157/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubSGD iter. 158/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubSGD iter. 159/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubSGD iter. 160/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubSGD iter. 161/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubSGD iter. 162/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubSGD iter. 163/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubSGD iter. 164/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubSGD iter. 165/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubSGD iter. 166/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubSGD iter. 167/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubSGD iter. 168/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubSGD iter. 169/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubSGD iter. 170/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubSGD iter. 171/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubSGD iter. 172/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubSGD iter. 173/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubSGD iter. 174/499: loss=1.3856276371044742, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubSGD iter. 175/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubSGD iter. 176/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubSGD iter. 177/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubSGD iter. 178/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubSGD iter. 179/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubSGD iter. 180/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubSGD iter. 181/499: loss=1.3856276371044747, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubSGD iter. 182/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubSGD iter. 183/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubSGD iter. 184/499: loss=1.3856276371044751, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubSGD iter. 185/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubSGD iter. 186/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubSGD iter. 187/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubSGD iter. 188/499: loss=1.385627637104474, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubSGD iter. 189/499: loss=1.385627637104475, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubSGD iter. 190/499: loss=1.3856276371044747, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubSGD iter. 191/499: loss=1.3925583301737823, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubSGD iter. 192/499: loss=1.3925583301737823, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubSGD iter. 193/499: loss=1.3925583301737816, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubSGD iter. 194/499: loss=1.3994890232430894, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubSGD iter. 195/499: loss=1.3994890232430897, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubSGD iter. 196/499: loss=1.3994890232430888, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubSGD iter. 197/499: loss=1.3994890232430897, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubSGD iter. 198/499: loss=1.4064197163123962, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubSGD iter. 199/499: loss=1.4064197163123973, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubSGD iter. 200/499: loss=1.4064197163123968, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubSGD iter. 201/499: loss=1.4133504093817044, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubSGD iter. 202/499: loss=1.4133504093817038, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubSGD iter. 203/499: loss=1.4133504093817044, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubSGD iter. 204/499: loss=1.4133504093817042, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubSGD iter. 205/499: loss=1.4202811024510116, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubSGD iter. 206/499: loss=1.420281102451012, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubSGD iter. 207/499: loss=1.4202811024510118, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubSGD iter. 208/499: loss=1.427211795520319, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubSGD iter. 209/499: loss=1.4272117955203192, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubSGD iter. 210/499: loss=1.4272117955203185, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubSGD iter. 211/499: loss=1.4272117955203194, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubSGD iter. 212/499: loss=1.434142488589626, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubSGD iter. 213/499: loss=1.4341424885896266, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubSGD iter. 214/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubSGD iter. 215/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubSGD iter. 216/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubSGD iter. 217/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubSGD iter. 218/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubSGD iter. 219/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubSGD iter. 220/499: loss=1.434142488589627, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubSGD iter. 221/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubSGD iter. 222/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubSGD iter. 223/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubSGD iter. 224/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubSGD iter. 225/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubSGD iter. 226/499: loss=1.4410731816589344, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubSGD iter. 227/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubSGD iter. 228/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubSGD iter. 229/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubSGD iter. 230/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubSGD iter. 231/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubSGD iter. 232/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubSGD iter. 233/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubSGD iter. 234/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubSGD iter. 235/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubSGD iter. 236/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubSGD iter. 237/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubSGD iter. 238/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubSGD iter. 239/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubSGD iter. 240/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubSGD iter. 241/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubSGD iter. 242/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubSGD iter. 243/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubSGD iter. 244/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubSGD iter. 245/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubSGD iter. 246/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubSGD iter. 247/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubSGD iter. 248/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubSGD iter. 249/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubSGD iter. 250/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubSGD iter. 251/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubSGD iter. 252/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubSGD iter. 253/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubSGD iter. 254/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubSGD iter. 255/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubSGD iter. 256/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubSGD iter. 257/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubSGD iter. 258/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubSGD iter. 259/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubSGD iter. 260/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubSGD iter. 261/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubSGD iter. 262/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubSGD iter. 263/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubSGD iter. 264/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubSGD iter. 265/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubSGD iter. 266/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubSGD iter. 267/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubSGD iter. 268/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubSGD iter. 269/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubSGD iter. 270/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubSGD iter. 271/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubSGD iter. 272/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubSGD iter. 273/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubSGD iter. 274/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubSGD iter. 275/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubSGD iter. 276/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubSGD iter. 277/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubSGD iter. 278/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubSGD iter. 279/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubSGD iter. 280/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubSGD iter. 281/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubSGD iter. 282/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubSGD iter. 283/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubSGD iter. 284/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubSGD iter. 285/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubSGD iter. 286/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubSGD iter. 287/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubSGD iter. 288/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubSGD iter. 289/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubSGD iter. 290/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubSGD iter. 291/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubSGD iter. 292/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubSGD iter. 293/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubSGD iter. 294/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubSGD iter. 295/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubSGD iter. 296/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubSGD iter. 297/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubSGD iter. 298/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubSGD iter. 299/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubSGD iter. 300/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubSGD iter. 301/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubSGD iter. 302/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubSGD iter. 303/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubSGD iter. 304/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubSGD iter. 305/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubSGD iter. 306/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubSGD iter. 307/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubSGD iter. 308/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubSGD iter. 309/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubSGD iter. 310/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubSGD iter. 311/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubSGD iter. 312/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubSGD iter. 313/499: loss=1.4410731816589344, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubSGD iter. 314/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubSGD iter. 315/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubSGD iter. 316/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubSGD iter. 317/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubSGD iter. 318/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubSGD iter. 319/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubSGD iter. 320/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubSGD iter. 321/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubSGD iter. 322/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubSGD iter. 323/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubSGD iter. 324/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubSGD iter. 325/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubSGD iter. 326/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubSGD iter. 327/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubSGD iter. 328/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubSGD iter. 329/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubSGD iter. 330/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubSGD iter. 331/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubSGD iter. 332/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubSGD iter. 333/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubSGD iter. 334/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubSGD iter. 335/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubSGD iter. 336/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubSGD iter. 337/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubSGD iter. 338/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubSGD iter. 339/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubSGD iter. 340/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubSGD iter. 341/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubSGD iter. 342/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubSGD iter. 343/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubSGD iter. 344/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubSGD iter. 345/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubSGD iter. 346/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubSGD iter. 347/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubSGD iter. 348/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubSGD iter. 349/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubSGD iter. 350/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubSGD iter. 351/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubSGD iter. 352/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubSGD iter. 353/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubSGD iter. 354/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubSGD iter. 355/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubSGD iter. 356/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubSGD iter. 357/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubSGD iter. 358/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubSGD iter. 359/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubSGD iter. 360/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubSGD iter. 361/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubSGD iter. 362/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubSGD iter. 363/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubSGD iter. 364/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubSGD iter. 365/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubSGD iter. 366/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubSGD iter. 367/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubSGD iter. 368/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubSGD iter. 369/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubSGD iter. 370/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubSGD iter. 371/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubSGD iter. 372/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubSGD iter. 373/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubSGD iter. 374/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubSGD iter. 375/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubSGD iter. 376/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubSGD iter. 377/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubSGD iter. 378/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubSGD iter. 379/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubSGD iter. 380/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubSGD iter. 381/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubSGD iter. 382/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubSGD iter. 383/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubSGD iter. 384/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubSGD iter. 385/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubSGD iter. 386/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubSGD iter. 387/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubSGD iter. 388/499: loss=1.441073181658933, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubSGD iter. 389/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubSGD iter. 390/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubSGD iter. 391/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubSGD iter. 392/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubSGD iter. 393/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubSGD iter. 394/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubSGD iter. 395/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubSGD iter. 396/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubSGD iter. 397/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubSGD iter. 398/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubSGD iter. 399/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubSGD iter. 400/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubSGD iter. 401/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubSGD iter. 402/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubSGD iter. 403/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubSGD iter. 404/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubSGD iter. 405/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubSGD iter. 406/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubSGD iter. 407/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubSGD iter. 408/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubSGD iter. 409/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubSGD iter. 410/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubSGD iter. 411/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubSGD iter. 412/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubSGD iter. 413/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubSGD iter. 414/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubSGD iter. 415/499: loss=1.4341424885896275, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubSGD iter. 416/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubSGD iter. 417/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubSGD iter. 418/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubSGD iter. 419/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubSGD iter. 420/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubSGD iter. 421/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubSGD iter. 422/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubSGD iter. 423/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubSGD iter. 424/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubSGD iter. 425/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubSGD iter. 426/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubSGD iter. 427/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubSGD iter. 428/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubSGD iter. 429/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubSGD iter. 430/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubSGD iter. 431/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubSGD iter. 432/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubSGD iter. 433/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubSGD iter. 434/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubSGD iter. 435/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubSGD iter. 436/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubSGD iter. 437/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubSGD iter. 438/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubSGD iter. 439/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubSGD iter. 440/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubSGD iter. 441/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubSGD iter. 442/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubSGD iter. 443/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubSGD iter. 444/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubSGD iter. 445/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubSGD iter. 446/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubSGD iter. 447/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubSGD iter. 448/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubSGD iter. 449/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubSGD iter. 450/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubSGD iter. 451/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubSGD iter. 452/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubSGD iter. 453/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubSGD iter. 454/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubSGD iter. 455/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubSGD iter. 456/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubSGD iter. 457/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubSGD iter. 458/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubSGD iter. 459/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubSGD iter. 460/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubSGD iter. 461/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubSGD iter. 462/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubSGD iter. 463/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubSGD iter. 464/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubSGD iter. 465/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubSGD iter. 466/499: loss=1.434142488589626, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubSGD iter. 467/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubSGD iter. 468/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubSGD iter. 469/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubSGD iter. 470/499: loss=1.4341424885896257, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubSGD iter. 471/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubSGD iter. 472/499: loss=1.441073181658933, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubSGD iter. 473/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubSGD iter. 474/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubSGD iter. 475/499: loss=1.4410731816589342, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubSGD iter. 476/499: loss=1.4410731816589342, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubSGD iter. 477/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubSGD iter. 478/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubSGD iter. 479/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubSGD iter. 480/499: loss=1.4341424885896266, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubSGD iter. 481/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubSGD iter. 482/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubSGD iter. 483/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubSGD iter. 484/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubSGD iter. 485/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubSGD iter. 486/499: loss=1.441073181658934, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubSGD iter. 487/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubSGD iter. 488/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubSGD iter. 489/499: loss=1.4410731816589333, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubSGD iter. 490/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubSGD iter. 491/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubSGD iter. 492/499: loss=1.441073181658934, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubSGD iter. 493/499: loss=1.4410731816589337, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubSGD iter. 494/499: loss=1.4341424885896268, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubSGD iter. 495/499: loss=1.4410731816589337, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubSGD iter. 496/499: loss=1.4410731816589344, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubSGD iter. 497/499: loss=1.4410731816589333, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubSGD iter. 498/499: loss=1.4341424885896263, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubSGD: execution time=0.108 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a94d47a52b42fd9f4d573fc49b1d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=500, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
